---
title: "Model"
format: html
editor: visual
---

### EDA

```{r}
# ==== EDA (robust to missing column names) ====
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)

df <- read_csv("dynamic_supply_chain_logistics_dataset.csv", show_col_types = FALSE) %>%
  mutate(timestamp = ymd_hms(timestamp, quiet = TRUE))

theme_set(theme_minimal(base_size = 9))
small_theme <- theme(
  plot.title  = element_text(size = 10, face = "bold"),
  axis.title  = element_text(size = 9),
  axis.text   = element_text(size = 8),
  strip.text  = element_text(size = 8),
  legend.text = element_text(size = 8),
  legend.title= element_text(size = 9)
)

# --- Identify key columns safely ---
time_col   <- intersect(c("timestamp", "datetime", "date", "time"), names(df))[1]
target_col <- intersect(c("shipping_costs", "target", "y"), names(df))[1]
if (is.na(target_col)) stop("No target column found. Expected one of: 'shipping_costs', 'target', 'y'.")

# 1) Quick overview
glimpse(df)
summary(df[[target_col]])

# 2) Target distribution
ggplot(df, aes(x = .data[[target_col]])) +
  geom_histogram(bins = 50) +
  labs(title = paste("Distribution of", target_col), x = target_col, y = "Count") +
  theme_minimal()

# 3) Time series trend (only if timestamp-like column exists)
if (!is.na(time_col)) {
  ggplot(df, aes(x = .data[[time_col]], y = .data[[target_col]])) +
    geom_line() +
    labs(title = paste(target_col, "Over Time"), x = time_col, y = target_col) +
    theme_minimal()
}

# ---- 4) Predictor distributions (FACETED — smaller text only) ----
num_pred <- df %>%
  select(where(is.numeric)) %>%
  select(-all_of(target_col))

if (ncol(num_pred) > 0) {
  num_long <- num_pred %>%
    pivot_longer(everything(), names_to = "feature", values_to = "value")

  ggplot(num_long, aes(x = value)) +
    geom_histogram(bins = 30) +
    facet_wrap(~ feature, scales = "free_x") +
    labs(title = "Distributions of Numeric Predictors", x = NULL, y = "Count") +
    theme_minimal(base_size = 8) +                                     # smaller base text
    theme(
      plot.title = element_text(size = 9, face = "bold"),
      axis.title = element_text(size = 8),
      axis.text  = element_text(size = 7),
      strip.text = element_text(size = 7)                              # smaller facet labels
    )
}

# 5) Relationships with target (scatter + linear trend)
if (ncol(num_pred) > 0) {
  rel_long <- bind_cols(df %>% select(all_of(target_col)), num_pred) %>%
    pivot_longer(-all_of(target_col), names_to = "feature", values_to = "value")

  ggplot(rel_long, aes(x = value, y = .data[[target_col]])) +
    geom_point(alpha = 0.35, size = 0.5) +
    geom_smooth(method = "lm", se = FALSE) +
    facet_wrap(~ feature, scales = "free_x") +
    labs(title = paste(target_col, "vs Numeric Predictors"), x = NULL, y = target_col) +
    theme_minimal()
}

# 6) Correlation heatmap (numeric only; drop constant columns)
num_for_cor <- df %>%
  select(where(is.numeric)) %>%
  select(where(~ sd(.x, na.rm = TRUE) > 0))

if (ncol(num_for_cor) > 1) {
  cor_mat <- cor(num_for_cor, use = "pairwise.complete.obs")
  corrplot(cor_mat,
           method = "color",
           type   = "upper",
           tl.col = "black",
           tl.cex = 0.5,
           mar    = c(0,0,1,0),
           title  = "Correlation Heatmap (Numeric Features)")
}
```

### Part 1 - Global Forecasting

```{r}
library(readr); library(dplyr); library(lubridate)

raw <- read_csv("dynamic_supply_chain_logistics_dataset.csv", show_col_types = FALSE) %>%
  mutate(timestamp = ymd_hms(timestamp, quiet = TRUE))

raw_gps <- raw %>%
  filter(!is.na(vehicle_gps_latitude), !is.na(vehicle_gps_longitude)) %>%
  select(timestamp, shipping_costs,
         traffic_congestion_level, port_congestion_level,
         fuel_consumption_rate, loading_unloading_time,
         supplier_reliability_score,
         vehicle_gps_latitude, vehicle_gps_longitude)

set.seed(42)
k <- 8
km <- kmeans(raw_gps[, c("vehicle_gps_latitude","vehicle_gps_longitude")], centers = k)
raw_gps$zone_id <- paste0("Z", km$cluster)

library(tidyr); library(zoo)

zone_hourly <- raw_gps %>%
  group_by(timestamp, zone_id) %>%
  summarise(
    shipping_costs = sum(shipping_costs, na.rm = TRUE),            
    traffic_congestion_level = mean(traffic_congestion_level, na.rm = TRUE),
    port_congestion_level    = mean(port_congestion_level, na.rm = TRUE),
    fuel_consumption_rate    = mean(fuel_consumption_rate, na.rm = TRUE),
    loading_unloading_time   = mean(loading_unloading_time, na.rm = TRUE),
    supplier_reliability_score = mean(supplier_reliability_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(zone_id, timestamp) %>%
  mutate(
    # time features
    hour  = hour(timestamp),
    dow   = wday(timestamp),
    month = month(timestamp),
    sin_hr = sin(2*pi*hour/24),  cos_hr = cos(2*pi*hour/24),
    sin_dw = sin(2*pi*dow/7),    cos_dw = cos(2*pi*dow/7),

    # zone-wise lags (computed within each zone)
    shipping_costs_lag1  = lag(shipping_costs, 1),
    shipping_costs_lag24 = lag(shipping_costs, 24)
  ) %>%
  group_by(zone_id) %>%
  drop_na(shipping_costs_lag1, shipping_costs_lag24) %>%
  ungroup()

zone_hourly_out <- zone_hourly %>%
  mutate(timestamp = format(timestamp, "%Y-%m-%d %H:%M:%S"))
write_csv(zone_hourly_out, "shipping_costs.csv")

```

```{r}
library(readr); library(dplyr); library(lubridate); library(zoo)

# 1) load
df <- read_csv("shipping_costs.csv", show_col_types = FALSE) %>%
  mutate(
    timestamp = ymd_hms(timestamp, quiet = TRUE),
    shipping_costs = as.numeric(shipping_costs)
  ) %>%
  arrange(timestamp)

# 2) richer lags + volatility
df <- df %>%
  mutate(
    # extra lags (short, daily, multi-day)
    lag1  = lag(shipping_costs, 1),
    lag2  = lag(shipping_costs, 2),
    lag3  = lag(shipping_costs, 3),
    lag6  = lag(shipping_costs, 6),
    lag12 = lag(shipping_costs, 12),
    lag24 = lag(shipping_costs, 24),
    lag48 = lag(shipping_costs, 48),
    lag72 = lag(shipping_costs, 72),

    # volatility / changes
    diff1  = shipping_costs - lag1,
    diff24 = shipping_costs - lag24,

    # rolling stats (24h)
    roll_mean_24 = rollmeanr(shipping_costs, 24, fill = NA),
    roll_sd_24   = rollapplyr(shipping_costs, 24, sd,  fill = NA),
    roll_max_24  = rollapplyr(shipping_costs, 24, max, fill = NA),
    roll_min_24  = rollapplyr(shipping_costs, 24, min, fill = NA)
  ) %>%
  tidyr::drop_na()  # drop initial rows without lags/rolls

# 3) split
n   <- nrow(df)
cut <- floor(0.8 * n)
train <- df[1:cut, ]
test  <- df[(cut+1):n, ]

# 4) features = everything except timestamp + raw target
X_cols <- setdiff(names(df), c("timestamp", "shipping_costs"))

```

```{r}
library(xgboost)
# ============================================
# Training Setup (Global XGBoost, no zone_id, with bst_full)
# ============================================

# remove non-numeric + zone_id
X_cols <- setdiff(names(train), c("timestamp", "shipping_costs", "zone_id"))

# ensure all predictors are numeric
train_x <- train[, X_cols]
train_x[] <- lapply(train_x, function(col) as.numeric(as.character(col)))

test_x <- test[, X_cols]
test_x[] <- lapply(test_x, function(col) as.numeric(as.character(col)))

# build matrices
dtrain <- xgb.DMatrix(as.matrix(train_x), label = log1p(train$shipping_costs))
dtest  <- xgb.DMatrix(as.matrix(test_x),  label = log1p(test$shipping_costs))

# params
params <- list(
  objective        = "reg:squarederror",
  eval_metric      = "rmse",
  max_depth        = 10,
  eta              = 0.05,
  subsample        = 0.9,
  colsample_bytree = 0.9
)

# 1) train with early stopping
bst <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 5000,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 50,
  print_every_n = 50
)

best_iter <- bst$best_iteration

# 2) retrain on full training data
dtrain_full <- xgb.DMatrix(as.matrix(train_x), label = log1p(train$shipping_costs))
bst_full <- xgb.train(
  params = params,
  data = dtrain_full,
  nrounds = best_iter,
  verbose = 0
)

# predictions from bst_full
preds  <- expm1(predict(bst_full, dtest))
actual <- test$shipping_costs

```

```{r}
library(ggplot2); library(tidyr); library(dplyr)

dtest <- xgb.DMatrix(as.matrix(test[, X_cols]))
preds_log <- predict(bst_full, dtest)
preds     <- expm1(preds_log)         
actual    <- test$shipping_costs

# metrics
eps <- 1e-9
rmse  <- sqrt(mean((preds - actual)^2, na.rm = TRUE))
mae   <- mean(abs(preds - actual), na.rm = TRUE)
mape  <- mean(abs((preds - actual) / pmax(actual, eps)), na.rm = TRUE) * 100
smape <- mean(2 * abs(preds - actual) / (abs(preds) + abs(actual) + eps), na.rm = TRUE) * 100
r2    <- {
  ss_res <- sum((actual - preds)^2, na.rm = TRUE)
  ss_tot <- sum((actual - mean(actual, na.rm = TRUE))^2, na.rm = TRUE)
  1 - ss_res/(ss_tot + eps)
}

eval_tbl <- data.frame(RMSE = rmse, MAE = mae, MAPE = mape, sMAPE = smape, R2 = r2)
print(eval_tbl)
readr::write_csv(eval_tbl, "metrics_hourly_log_highcap.csv")

pred_df <- tibble::tibble(
  timestamp = test$timestamp,
  actual = actual,
  predicted = preds
)

# simple overlay plot 
plot_df <- pred_df %>% pivot_longer(c(actual, predicted), names_to="series", values_to="value")
p <- ggplot(plot_df, aes(timestamp, value, colour = series)) +
  geom_line(alpha = 0.85) +
  labs(title = "Shipping costs — actual vs predicted (hourly, log-target)",
       x = "Time", y = "Shipping costs", colour = "") +
  theme_minimal() +
  theme(legend.position = "bottom")
print(p)


```

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

# build plotting dataframe
plot_df <- data.frame(
  timestamp = test$timestamp,
  actual    = actual,
  predicted = preds
)

# plot: predicted line + actual scatter
p <- ggplot(plot_df, aes(x = timestamp)) +
  geom_point(aes(y = actual), colour = "black", alpha = 0.6, size = 1.2) +
  geom_line(aes(y = predicted, colour = "Predicted"), size = 0.9) +
  labs(
    title = "Global Model: Actual (scatter) vs Predicted (line)",
    x = "Time",
    y = "Shipping Costs",
    colour = ""
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggsave("global_actual_vs_predicted.png", p, width = 10, height = 5, dpi = 150)
print(p)

```

### Part 2- Zone-Level Forecasting

```{r}
library(readr); library(dplyr); library(lubridate)

raw <- read_csv("dynamic_supply_chain_logistics_dataset.csv", show_col_types = FALSE) %>%
  mutate(timestamp = ymd_hms(timestamp, quiet = TRUE))

# keep rows with GPS
raw_gps <- raw %>%
  filter(!is.na(vehicle_gps_latitude), !is.na(vehicle_gps_longitude)) %>%
  select(timestamp, shipping_costs,
         traffic_congestion_level, port_congestion_level,
         fuel_consumption_rate, loading_unloading_time,
         supplier_reliability_score,
         vehicle_gps_latitude, vehicle_gps_longitude)

# cluster GPS into zones at the ROW level (no join-on-timestamp)
set.seed(42)
k <- 8  # adjust if you want
km <- kmeans(raw_gps[, c("vehicle_gps_latitude","vehicle_gps_longitude")], centers = k)
raw_gps$zone_id <- paste0("Z", km$cluster)

library(tidyr); library(zoo)

zone_hourly <- raw_gps %>%
  group_by(timestamp, zone_id) %>%
  summarise(
    shipping_costs = sum(shipping_costs, na.rm = TRUE),             # sum cost per zone-hour
    traffic_congestion_level = mean(traffic_congestion_level, na.rm = TRUE),
    port_congestion_level    = mean(port_congestion_level, na.rm = TRUE),
    fuel_consumption_rate    = mean(fuel_consumption_rate, na.rm = TRUE),
    loading_unloading_time   = mean(loading_unloading_time, na.rm = TRUE),
    supplier_reliability_score = mean(supplier_reliability_score, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(zone_id, timestamp) %>%
  mutate(
    # time features
    hour  = hour(timestamp),
    dow   = wday(timestamp),
    month = month(timestamp),
    sin_hr = sin(2*pi*hour/24),  cos_hr = cos(2*pi*hour/24),
    sin_dw = sin(2*pi*dow/7),    cos_dw = cos(2*pi*dow/7),

    # zone-wise lags (computed within each zone)
    shipping_costs_lag1  = lag(shipping_costs, 1),
    shipping_costs_lag24 = lag(shipping_costs, 24)
  ) %>%
  group_by(zone_id) %>%
  # drop initial rows without lags per zone
  drop_na(shipping_costs_lag1, shipping_costs_lag24) %>%
  ungroup()

# write a clean timestamp without "Z"
zone_hourly_out <- zone_hourly %>%
  mutate(timestamp = format(timestamp, "%Y-%m-%d %H:%M:%S"))
write_csv(zone_hourly_out, "shipping_costs_zone_hourly.csv")
```

```{r}
# ============================================
# Global XGBoost with zones
# ============================================
library(readr); library(dplyr); library(lubridate); library(zoo)
library(xgboost); library(ggplot2); library(tidyr)

# ---- 1) Load zone-level data ----
df <- read_csv("shipping_costs_zone_hourly.csv", show_col_types = FALSE) %>%
  mutate(timestamp = ymd_hms(timestamp, quiet = TRUE)) %>%
  arrange(zone_id, timestamp)

# ---- 2) Add richer features per zone ----
df <- df %>%
  group_by(zone_id) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  mutate(
    lag1  = lag(shipping_costs, 1),
    lag2  = lag(shipping_costs, 2),
    lag3  = lag(shipping_costs, 3),
    lag6  = lag(shipping_costs, 6),
    lag12 = lag(shipping_costs, 12),
    lag24 = lag(shipping_costs, 24),
    lag48 = lag(shipping_costs, 48),
    lag72 = lag(shipping_costs, 72),

    diff1  = shipping_costs - lag1,
    diff24 = shipping_costs - lag24,

    roll_mean_24 = rollmeanr(shipping_costs, 24, fill = NA),
    roll_sd_24   = rollapplyr(shipping_costs, 24, sd,  fill = NA),
    roll_max_24  = rollapplyr(shipping_costs, 24, max, fill = NA),
    roll_min_24  = rollapplyr(shipping_costs, 24, min, fill = NA)
  ) %>%
  ungroup() %>%
  tidyr::drop_na()

# ---- 3) One-hot encode zone_id BUT keep original ----
zone_dummies <- model.matrix(~ zone_id - 1, data = df)
df_enc <- cbind(df, zone_dummies)   # keep original zone_id column

# ---- 4) Train/test split (by time) ----
n   <- nrow(df_enc)
# ---- 4) Train/test split (by time, across all zones) ----
time_cut <- quantile(df_enc$timestamp, 0.8)   # 80% by time

train <- df_enc %>% filter(timestamp <= time_cut)
test  <- df_enc %>% filter(timestamp >  time_cut)

X_cols <- setdiff(names(df_enc), c("timestamp","shipping_costs","zone_id"))


X_cols <- setdiff(names(df_enc), c("timestamp","shipping_costs","zone_id"))

# validation from last 10% of training
ntr <- nrow(train); vcut <- floor(0.9 * ntr)
tr <- train[1:vcut, ]; va <- train[(vcut+1):ntr, ]

dtr <- xgb.DMatrix(as.matrix(tr[, X_cols]), label = log1p(tr$shipping_costs))
dva <- xgb.DMatrix(as.matrix(va[, X_cols]), label = log1p(va$shipping_costs))
```

```{r}
# ---- 5) Train model (high capacity + log target) ----
params <- list(
  objective        = "reg:squarederror",
  eval_metric      = "rmse",
  max_depth        = 12,
  eta              = 0.03,
  subsample        = 1.0,
  colsample_bytree = 1.0,
  min_child_weight = 1,
  lambda           = 0,
  alpha            = 0,
  gamma            = 0
)

bst <- xgb.train(
  params = params,
  data = dtr,
  nrounds = 5000,
  watchlist = list(train = dtr, valid = dva),
  early_stopping_rounds = 200,
  print_every_n = 50
)

# Refit
dtrain <- xgb.DMatrix(as.matrix(train[, X_cols]), label = log1p(train$shipping_costs))
bst_full <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = bst$best_iteration,
  verbose = 0
)

# ---- 6) Predictions ----
dtest <- xgb.DMatrix(as.matrix(test[, X_cols]))
preds <- expm1(predict(bst_full, dtest))
actual <- test$shipping_costs

pred_df <- tibble::tibble(
  timestamp = test$timestamp,
  zone_id   = test$zone_id,   
  actual    = actual,
  predicted = preds
)

write_csv(pred_df, "zone_predictions_highcap.csv")
global_metrics <- metric_fun(pred_df$predicted, pred_df$actual)
print(global_metrics)


# ---- 7) Metrics ----
eps <- 1e-9
metric_fun <- function(pred, act) {
  data.frame(
    RMSE  = sqrt(mean((pred - act)^2, na.rm = TRUE)),
    MAE   = mean(abs(pred - act), na.rm = TRUE),
    MAPE  = mean(abs((pred - act) / pmax(abs(act), eps)), na.rm = TRUE) * 100,
    sMAPE = mean(2 * abs(pred - act) / (abs(pred) + abs(act) + eps), na.rm = TRUE) * 100,
    R2    = {
      ss_res <- sum((act - pred)^2, na.rm = TRUE)
      ss_tot <- sum((act - mean(act, na.rm = TRUE))^2, na.rm = TRUE)
      1 - ss_res / (ss_tot + eps)
    }
  )
}

zone_metrics <- pred_df %>%
  group_by(zone_id) %>%
  summarise(metric_fun(predicted, actual), .groups="drop")

write_csv(zone_metrics, "zone_metrics_highcap.csv")
print(zone_metrics)

# ---- 8) Plot (faceted per zone) ----
plot_df <- pred_df %>%
  pivot_longer(cols = c(actual, predicted), names_to = "series", values_to = "value")

p <- ggplot(plot_df, aes(timestamp, value, colour = series)) +
  geom_line(alpha=0.8) +
  facet_wrap(~ zone_id, scales="free_y") +
  labs(title="Zone-level forecasts (log target, high-capacity XGB)",
       x="Time", y="Shipping Costs", colour="") +
  theme_minimal() +
  theme(legend.position="bottom")

ggsave("zone_forecasts_highcap.png", p, width=12, height=8, dpi=150)

```

### Part 3 - Forecasting Reconciliation (BU, TD, Shrinkage)

Comparison of reconciliation methods and their impact on coherence and accuracy.

```{r}
# ============================================
# A) Setup
# ============================================
library(readr); library(dplyr); library(tidyr); library(lubridate); library(ggplot2)

pred_df <- read_csv("zone_predictions_highcap.csv", show_col_types = FALSE) %>%
  mutate(timestamp = ymd_hms(timestamp, quiet = TRUE)) %>%
  arrange(timestamp, zone_id)

pred_wide <- pred_df %>%
  select(timestamp, zone_id, actual, predicted) %>%
  pivot_wider(
    id_cols = timestamp,
    names_from = zone_id,
    values_from = c(actual, predicted)
  ) %>%
  arrange(timestamp)

Yhat_B <- as.matrix(pred_wide %>% select(starts_with("predicted_")))
Y_act  <- as.matrix(pred_wide %>% select(starts_with("actual_")))
timestamps <- pred_wide$timestamp

zones <- gsub("predicted_", "", colnames(Yhat_B))
colnames(Yhat_B) <- zones
colnames(Y_act)  <- zones
```

```{r}

# ============================================
# B) Reconciliation
# ============================================

# ---- Bottom-Up ----
Yhat_bu_bottom <- Yhat_B
Yhat_bu_total  <- rowSums(Yhat_bu_bottom, na.rm=TRUE)

# ---- Top-Down ----
Yhat_total <- rowSums(Yhat_B, na.rm=TRUE)
zone_means <- colMeans(Y_act, na.rm=TRUE)

if (all(is.na(zone_means)) || sum(zone_means, na.rm=TRUE) <= 0) {
  props <- rep(1/length(zone_means), length(zone_means))
} else {
  props <- zone_means / sum(zone_means, na.rm=TRUE)
}
props[is.na(props)] <- 1/length(props)
props <- props / sum(props)

Yhat_td_bottom <- sweep(matrix(Yhat_total, nrow=length(Yhat_total), ncol=length(props)),
                        2, props, `*`)
Yhat_td_total <- rowSums(Yhat_td_bottom)

# ---- Shrinkage ----
alpha <- 0.5
Yhat_shrink_bottom <- alpha * Yhat_bu_bottom + (1 - alpha) * Yhat_td_bottom
Yhat_shrink_total  <- rowSums(Yhat_shrink_bottom)


```

```{r}

# ============================================
# C) Metrics + Plots (safe)
# ============================================

eps <- 1e-9
metric_fun <- function(pred, act) {
  # Replace NA with 0
  act <- ifelse(is.na(act), 0, act)
  pred <- ifelse(is.na(pred), 0, pred)

  ss_res <- sum((act - pred)^2, na.rm=TRUE)
  ss_tot <- sum((act - mean(act, na.rm=TRUE))^2, na.rm=TRUE)

  data.frame(
    RMSE  = sqrt(mean((pred-act)^2, na.rm=TRUE)),
    MAE   = mean(abs(pred-act), na.rm=TRUE),
    MAPE  = ifelse(sum(abs(act), na.rm=TRUE) == 0, NA,
                   mean(abs((pred-act)/(act+eps)), na.rm=TRUE) * 100),
    sMAPE = mean(2*abs(pred-act)/(abs(pred)+abs(act)+eps), na.rm=TRUE) * 100,
    R2    = ifelse(ss_tot == 0, NA, 1 - ss_res/(ss_tot+eps))
  )
}

act_long <- as.data.frame(Y_act)
colnames(act_long) <- zones
act_long <- cbind(timestamp=timestamps, act_long) %>%
  pivot_longer(-timestamp, names_to="zone_id", values_to="actual")

long_bu <- cbind(timestamp=timestamps, as.data.frame(Yhat_bu_bottom)) %>%
  pivot_longer(-timestamp, names_to="zone_id", values_to="pred_bu")

long_td <- cbind(timestamp=timestamps, as.data.frame(Yhat_td_bottom)) %>%
  pivot_longer(-timestamp, names_to="zone_id", values_to="pred_td")

long_shrink <- cbind(timestamp=timestamps, as.data.frame(Yhat_shrink_bottom)) %>%
  pivot_longer(-timestamp, names_to="zone_id", values_to="pred_shrink")

compare <- act_long %>%
  left_join(long_bu, by=c("timestamp","zone_id")) %>%
  left_join(long_td, by=c("timestamp","zone_id")) %>%
  left_join(long_shrink, by=c("timestamp","zone_id"))

zone_metrics <- compare %>%
  group_by(zone_id) %>%
  summarise(
    metric_fun(pred_bu,     actual) %>% setNames(paste0(names(.),"_BU")),
    metric_fun(pred_td,     actual) %>% setNames(paste0(names(.),"_TD")),
    metric_fun(pred_shrink, actual) %>% setNames(paste0(names(.),"_Shrink")),
    .groups="drop"
)

write_csv(zone_metrics, "zone_metrics_reconciled_simple.csv")
print(zone_metrics)

totals <- data.frame(
  actual = rowSums(Y_act, na.rm=TRUE),
  BU     = Yhat_bu_total,
  TD     = Yhat_td_total,
  Shrink = Yhat_shrink_total
)

total_metrics <- bind_rows(
  metric_fun(totals$BU, totals$actual)     %>% mutate(method="BU"),
  metric_fun(totals$TD, totals$actual)     %>% mutate(method="TD"),
  metric_fun(totals$Shrink, totals$actual) %>% mutate(method="Shrink")
)

print(total_metrics)



# ---- BU plot ----
p_bu <- ggplot(compare, aes(timestamp)) +
  geom_line(aes(y=actual), colour="black", linetype="dashed", size=0.6) +
  geom_line(aes(y=pred_bu, colour="BU"), size=0.8) +
  facet_wrap(~ zone_id, scales="free_y") +
  labs(title="Bottom-Up vs Actual", y="Shipping Costs", x="Time", colour="") +
  theme_minimal() +
  theme(legend.position="bottom")
ggsave("zone_forecasts_BU.png", p_bu, width=12, height=8, dpi=150)

# ---- TD plot ----
p_td <- ggplot(compare, aes(timestamp)) +
  geom_line(aes(y=actual), colour="black", linetype="dashed", size=0.6) +
  geom_line(aes(y=pred_td, colour="TD"), size=0.8) +
  facet_wrap(~ zone_id, scales="free_y") +
  labs(title="Top-Down vs Actual", y="Shipping Costs", x="Time", colour="") +
  theme_minimal() +
  theme(legend.position="bottom")
ggsave("zone_forecasts_TD.png", p_td, width=12, height=8, dpi=150)

# ---- Shrinkage plot ----
p_shrink <- ggplot(compare, aes(timestamp)) +
  geom_line(aes(y=actual), colour="black", linetype="dashed", size=0.6) +
  geom_line(aes(y=pred_shrink, colour="Shrinkage"), size=0.8) +
  facet_wrap(~ zone_id, scales="free_y") +
  labs(title="Shrinkage vs Actual", y="Shipping Costs", x="Time", colour="") +
  theme_minimal() +
  theme(legend.position="bottom")
ggsave("zone_forecasts_Shrinkage.png", p_shrink, width=12, height=8, dpi=150)

```
